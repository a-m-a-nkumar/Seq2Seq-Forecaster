{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Dropout, LSTM, GRU, TimeDistributed, Input\nfrom keras.optimizers import SGD\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\n\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nfrom mpl_toolkits.mplot3d import Axes3D\nimport numpy as np\nimport pandas as pd\nimport datetime\n\nfrom sklearn.model_selection import cross_val_score, train_test_split\nfrom sklearn.model_selection import cross_validate,GridSearchCV\nfrom sklearn.feature_extraction import DictVectorizer\nfrom sklearn.utils.multiclass import unique_labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"markdown","source":"# Load datasets and cleaning\ntrain: 2016-01-01 : 2017-04-22\n\ntest: 2017-04-23 : 2017-05-31"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(\"train_final.csv\")\ntest = pd.read_csv(\"test_final.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Here I got rid of some features that were considered having duplicated contribution to the model training\ntrain_df = train_df.drop(columns=[ 'population', 'reserve_visitors', 'days_diff', 'day', 'season'])\ntest = test.drop(columns=['population', 'reserve_visitors','days_diff', 'day', 'season'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# refine column names\ntrain_df = train_df.rename({'visitors_x': 'visitors'}, axis = 1)\ntrain_df = train_df.rename({'day_of_week_y': 'day_of_week'}, axis = 1)\ntrain_df = train_df.rename({'month_y': 'month'}, axis = 1)\ntrain_df = train_df.rename({'longitude_y': 'longitude'}, axis = 1)\ntrain_df = train_df.rename({'latitude_y': 'latitude'}, axis = 1)\ntest = test.rename({'latitude_y': 'latitude'}, axis = 1)\ntest = test.rename({'longitude_y': 'longitude'}, axis = 1)\ntest = test.rename({'month_y': 'month'}, axis = 1)\ntest = test.rename({'day_of_week_y': 'day_of_week'}, axis = 1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = train_df.loc[:, ~train_df.columns.str.contains('^Unnamed')]\ntrain_df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = test.loc[:, ~test.columns.str.contains('^Unnamed')]\ntest.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Encode categorical columns\nCategorical columns: 'Food_Type','season', 'day_of_week', 'air_store_id'\n\nOne-hot encoding may provide better result, but I applied labels encoding to avoid high dimensional feature space."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Weekday\nle_weekday = LabelEncoder()\nle_weekday.fit(train_df['day_of_week'])\ntrain_df['day_of_week'] = le_weekday.transform(train_df['day_of_week'])\ntest['day_of_week'] = le_weekday.transform(test['day_of_week'])\n\n# id\nle_id = LabelEncoder()\nle_id.fit(train_df['air_store_id'])\ntrain_df['air_store_id'] = le_id.transform(train_df['air_store_id'])\ntest['air_store_id'] = le_id.transform(test['air_store_id'])\n\n# food type\nle_ftype = LabelEncoder()\nle_ftype.fit(train_df['Food_Type'])\ntrain_df['Food_Type'] = le_ftype.transform(train_df['Food_Type'])\ntest['Food_Type'] = le_ftype.transform(test['Food_Type'])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Fill the cells of missing values"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = train_df.fillna(-1)\ntest = test.fillna(-1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Input preparation for Seq2Seq modeling\n\nTime-independent features (autocorrelations, country, etc) are \"stretched\" to timeseries length."},{"metadata":{"trusted":true},"cell_type":"code","source":"# combine train and test sets\nX_all = train_df.append(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# date table (includes all dates for training and test period)\ndates = np.arange(np.datetime64(X_all.visit_date.min()),\n                  np.datetime64(X_all.visit_date.max()) + 1,\n                  datetime.timedelta(days=1))\nids = X_all['air_store_id'].unique()\ndates_all = dates.tolist()*len(ids)\nids_all = np.repeat(ids, len(dates.tolist())).tolist()\ndf_all = pd.DataFrame({\"air_store_id\": ids_all, \"visit_date\": dates_all})\ndf_all['visit_date'] = df_all['visit_date'].copy().apply(lambda x: str(x)[:10])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Data relevant to 'visit_date'"},{"metadata":{"trusted":true},"cell_type":"code","source":"# create copy of X_all with data relevant to 'visit_date'\nX_dates = X_all[['visit_date', 'year','month','week', 'is_holiday','next_day','prev_day',\\\n                 'daysToPrev25th','day_of_week','Consecutive_holidays']].copy()\n\n# remove duplicates to avoid memory issues\nX_dates = X_dates.drop_duplicates('visit_date')\n\n# merge dataframe that represents all dates per each restaurant with information about each date\ndf_to_reshape = df_all.merge(X_dates,\n                             how = \"left\",\n                             left_on = 'visit_date',\n                             right_on = 'visit_date')\nprint(df_to_reshape.columns)\nprint(df_to_reshape.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Data relevant to 'air_store_id'"},{"metadata":{"trusted":true},"cell_type":"code","source":"# create copy of X_all with data relevant to 'air_store_id'\nX_stores = X_all[['air_store_id', 'Food_Type', 'latitude','longitude']].copy()       \n\n# remove duplicates to avoid memory issues\nX_stores = X_stores.drop_duplicates('air_store_id')\n\n# merge dataframe that represents all dates per each restaurant with information about each restaurant\ndf_to_reshape = df_to_reshape.merge(X_stores,\n                                    how = \"left\",\n                                    left_on = 'air_store_id',\n                                    right_on = 'air_store_id')\nprint(df_to_reshape.columns)\nprint(df_to_reshape.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Data relevant to 'air_store_id'"},{"metadata":{"trusted":true},"cell_type":"code","source":"# merge dataframe that represents all dates per each restaurant with inf. about each restaurant per specific date\ndf_to_reshape = df_to_reshape.merge(X_all[['air_store_id', 'visit_date','prev_visitors', 'mean_visitors', \n                                       'median_visitors', 'max_visitors', 'min_visitors', 'count_observations','visitors']],\n                                    how = \"left\",\n                                    left_on = ['air_store_id', 'visit_date'],\n                                    right_on = ['air_store_id', 'visit_date'])\nprint(df_to_reshape.columns)\nprint(df_to_reshape.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# separate 'visitors' into output array\nY_lstm_df = df_to_reshape[['visit_date', 'air_store_id', 'visitors']].copy().fillna(0)\n\n# take log(y+1)\nY_lstm_df['visitors'] = np.log1p(Y_lstm_df['visitors'].values)\n\n# add flag for days when a restaurant was closed\ndf_to_reshape['closed_flag'] = np.where(df_to_reshape['visitors'].isnull() &\n                                        df_to_reshape['visit_date'].isin(train_df['visit_date']).values,1,0)\n\n# drop 'visitors' and from dataset\ndf_to_reshape = df_to_reshape.drop(['visitors'], axis = 1)\n\n# fill in NaN values\ndf_to_reshape = df_to_reshape.fillna(-1)\n\n# list of df_to_reshape columns without 'air_store_id' and 'visit_date'\ncolumns_list = [x for x in list(df_to_reshape.iloc[:,2:])]\n\n# bound all numerical values between -1 and 1\n# note: to avoid data leakage 'fit' should be made on traid data and 'transform' on train and test data\n# in this case all data in test set is taken from train set, thus fit/transform on all data \nscaler = MinMaxScaler(feature_range=(-1, 1))\nscaler.fit(df_to_reshape[columns_list])\ndf_to_reshape[columns_list] = scaler.transform(df_to_reshape[columns_list])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# SPECIFIC PREPARATION FOR NEURAL NETWORK AND ENCODER/DECODER ---------------\n# reshape X into (samples, timesteps, features)\nX_all_lstm = df_to_reshape.values[:,2:].reshape(len(ids),\n                                                len(dates),\n                                                df_to_reshape.shape[1]-2)\n\n# isolate output for train set and reshape it for time series\nY_lstm_df = Y_lstm_df.loc[Y_lstm_df['visit_date'].isin(train_df['visit_date'].values) &\n                          Y_lstm_df['air_store_id'].isin(train_df['air_store_id'].values),]\nY_lstm = Y_lstm_df.values[:,2].reshape(len(train_df['air_store_id'].unique()),\n                                       len(train_df['visit_date'].unique()),\n                                       1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# test dates\nn_test_dates = len(test['visit_date'].unique())\n\n# make additional features for number of visitors in t-1, t-2, ... t-7\nt_minus = np.ones([Y_lstm.shape[0],Y_lstm.shape[1],1])\nfor i in range(1,8):\n    temp = Y_lstm.copy()\n    temp[:,i:,:] = Y_lstm[:,0:-i,:].copy()\n    t_minus = np.concatenate((t_minus[...], temp[...]), axis = 2)\nt_minus = t_minus[:,:,1:]\nprint (\"t_minus shape\", t_minus.shape)\n\n\n# split X_all into training and test data\nX_lstm = X_all_lstm[:,:-n_test_dates,:]\nX_lstm_test = X_all_lstm[:,-n_test_dates:,:]\n\n# add t-1, t-2 ... t-7 visitors to feature vector\nX_lstm = np.concatenate((X_lstm[...], t_minus[...]), axis = 2)\n\n# split training set into train and validation sets\nX_tr = X_lstm[:,39:-140,:]\nY_tr = Y_lstm[:,39:-140,:]\n\nX_val = X_lstm[:,-140:,:]\nY_val = Y_lstm[:,-140:,:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Encoder-Decoder Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# MODEL FOR ENCODER AND DECODER -------------------------------------------\nnum_encoder_tokens = X_lstm.shape[2]\nlatent_dim = 256 \n\n# encoder training\nencoder_inputs = Input(shape = (None, num_encoder_tokens))\nencoder = LSTM(latent_dim, \n               batch_input_shape = (1, None, num_encoder_tokens),\n               stateful = False,\n               return_sequences = True,\n               return_state = True,\n               recurrent_initializer = 'glorot_uniform')\n\nencoder_outputs, state_h, state_c = encoder(encoder_inputs)\n# We discard `encoder_outputs` and only keep the states.\nencoder_states = [state_h, state_c] \n\n# Decoder training, using 'encoder_states' as initial state.\ndecoder_inputs = Input(shape=(None, num_encoder_tokens))\n\ndecoder_lstm_1 = LSTM(latent_dim,\n                      batch_input_shape = (1, None, num_encoder_tokens),\n                      stateful = False,\n                      return_sequences = True,\n                      return_state = False,\n                      dropout = 0.4,\n                      recurrent_dropout = 0.4) # True\n\ndecoder_lstm_2 = LSTM(128, \n                     stateful = False,\n                     return_sequences = True,\n                     return_state = True,\n                     dropout = 0.4,\n                     recurrent_dropout = 0.4)\n\ndecoder_outputs, _, _ = decoder_lstm_2(decoder_lstm_1(decoder_inputs, initial_state = encoder_states))\ndecoder_dense = TimeDistributed(Dense(Y_lstm.shape[2], activation = 'relu'))\ndecoder_outputs = decoder_dense(decoder_outputs)\n\n# Define the model that will turn\n# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\ntraining_model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n# Train the model\ntraining_model.compile(optimizer = 'adam', loss = 'mean_squared_error')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# useful for understanding the model architecture\ntraining_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# GENERATOR APPLIED TO FEED ENCODER AND DECODER ---------------------------\n# generator that randomly creates times series of 39 consecutive days\n# theses time series has following 3d shape: 829 restaurants * 39 days * num_features \ndef dec_enc_n_days_gen(X_3d, Y_3d, length):\n    while 1:\n        decoder_boundary = X_3d.shape[1] - length - 1\n        \n        encoder_start = np.random.randint(0, decoder_boundary)\n        encoder_end = encoder_start + length\n        \n        decoder_start = encoder_start + 1\n        decoder_end = encoder_end + 1\n        \n        X_to_conc = X_3d[:, encoder_start:encoder_end, :]\n        Y_to_conc = Y_3d[:, encoder_start:encoder_end, :]\n        X_to_decode = X_3d[:, decoder_start:decoder_end, :]\n        Y_decoder = Y_3d[:, decoder_start:decoder_end, :]\n        \n        yield([X_to_conc,\n               X_to_decode],\n               Y_decoder)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# TRAINING -------------------------------------------------------------\n# Training on X_tr/Y_tr and validate with X_val/Y_val\n# To perform validation training on validation data should be\n# made instead of training on full data set.\n# Then validation check is made on period outside of training data\n# (included in code below).\n\ntraining_model.fit_generator(dec_enc_n_days_gen(X_tr, Y_tr, 39),\n                             validation_data = dec_enc_n_days_gen(X_val, Y_val, 39),\n                             steps_per_epoch = X_lstm.shape[0],\n                             validation_steps = X_val.shape[0],\n                             verbose = 1,\n                             epochs = 5)\n\n\n# # Training on full dataset\n# training_model.fit_generator(dec_enc_n_days_gen(X_lstm[:,:,:], Y_lstm[:,:,:], 39),\n#                             steps_per_epoch = X_lstm[:,:,:].shape[0],\n#                             verbose = 1,\n#                             epochs = 5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# PREDICTION FUNCTION --------------------------------------------------\n\n# function takes 39 days before first prediction day (input_seq)\n# then using encoder to identify hidden states for these 39 days.\n# Next, decoder takes hidden states provided by encoder\n# and predicts number of visitors from day 2 to day 40.\n# Day 40 is the first day of target_seq.\n\n# Predicted value for day 40 is appended to features of day 41.\n# Then function takes period from day 2 to day 40 and repeat the process\n# unil all days in target sequence get their predictions. \n\n# The output of the function is the vector with predictions that has\n# following shape: 820 restaurants * 39 days * 1 predicted visitors amount\n\ndef predict_sequence(inf_enc, inf_dec, input_seq, Y_input_seq, target_seq):\n    # state of input sequence produced by encoder\n    state = inf_enc.predict(input_seq)\n    \n    # restrict target sequence to the same shape as X_lstm_test\n    target_seq = target_seq[:,:, :X_lstm_test.shape[2]]\n    \n    \n    # create vector that contains y for previous 7 days\n    t_minus_seq = np.concatenate((Y_input_seq[:,-1:,:], input_seq[:,-1:, X_lstm_test.shape[2]:-1]), axis = 2)\n    \n    # current sequence that is going to be modified each iteration of the prediction loop\n    current_seq = input_seq.copy()\n    \n    \n    # predicting outputs\n    output = np.ones([target_seq.shape[0],1,1])\n    for i in range(target_seq.shape[1]):\n        # add visitors for previous 7 days into features of a new day\n        new_day_features = np.concatenate((target_seq[:,i:i+1,:], t_minus_seq[...]), axis = 2)\n        \n        # move prediction window one day forward\n        current_seq = np.concatenate((current_seq[:,1:,:], new_day_features[:,]), axis = 1)\n        \n        \n        # predict visitors amount\n        pred = inf_dec.predict([current_seq] + state)\n        \n        # update t_minus_seq\n        t_minus_seq = np.concatenate((pred[:,-1:,:], t_minus_seq[...]), axis = 2)\n        t_minus_seq = t_minus_seq[:,:,:-1]        \n        \n        # update predicitons list\n        output = np.concatenate((output[...], pred[:,-1:,:]), axis = 1)\n        \n        # update state\n        state = inf_enc.predict(current_seq)\n    \n    return output[:,1:,:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# INFERENCE ENCODER AND DECODER -----------------------------------------    \n# inference encoder\nencoder_model = Model(encoder_inputs, encoder_states)\n\n# inference decoder\ndecoder_state_input_h = Input(shape=(latent_dim,))\ndecoder_state_input_c = Input(shape=(latent_dim,))\ndecoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\ndecoder_outputs,_,_ = decoder_lstm_2(decoder_lstm_1(decoder_inputs,\n                                                    initial_state = decoder_states_inputs))\ndecoder_outputs = decoder_dense(decoder_outputs)\ndecoder_model = Model([decoder_inputs] + decoder_states_inputs,\n                      [decoder_outputs])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predicting test values\nenc_dec_pred = predict_sequence(encoder_model,\n                                decoder_model,\n                                X_lstm[:,-X_lstm_test.shape[1]:,:],\n                                Y_lstm[:,-X_lstm_test.shape[1]:,:],\n                                X_lstm_test[:,:,:])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# test data\nsample_sub = pd.read_csv('sample_submission.csv')\n# transform test data\nair_test = sample_sub.copy()\nair_test['air_store_id'] = air_test['id'].apply(lambda x: str(x)[:-11])\nair_test['visit_date'] = air_test['id'].apply(lambda x: str(x)[-10:])\n\n# dataframe for predictions\nsubmission_lstm = air_test.copy()\nsubmission_lstm.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Add predicted test values to submission dataset ---------------------\n\n# Note: it is important to preserve the order of time series.\n# Thus, test set will contain all 829 lines in the same order as train set.\n# To make this 'air_store_id' is taken as in X and not in X_test (second line of 'test' variable below).\n# Only relevant results will be merged for submission dataframe\ntest_df = df_to_reshape.loc[df_to_reshape['visit_date'].isin(test['visit_date'].values) &\n                         df_to_reshape['air_store_id'].isin(train_df['air_store_id'].values),]\n\n\n# reshape predicted values to initial shape\ntest_pred = enc_dec_pred.reshape(test_df.shape[0], 1)\ntest_pred_exp = np.exp(test_pred) - 1.0\ntest_pred_exp[test_pred_exp<0] = 0\n\n# add predictions to dataframe with 'air_store_id' and 'visit_date'\ntest_df_pred = test_df[['air_store_id', 'visit_date']].copy()\ntest_df_pred['predicted'] = test_pred_exp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# reverse transform of 'air_store_id'\ntest_df_pred['air_store_id'] = le_id.inverse_transform(test_df_pred['air_store_id'])\n\n# finalizing submission csv file\nsubmission_df = submission_lstm.merge(test_df_pred,\n                                     how = 'left',\n                                     left_on = ['air_store_id', 'visit_date'],\n                                     right_on = ['air_store_id', 'visit_date'])\n\nsubmission_df['visitors'] = submission_df['predicted']\nsubmission_df = submission_df.drop(['air_store_id', 'visit_date', 'predicted'], axis = 1)\nsubmission_df.to_csv('submission5.csv', index = False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":1}